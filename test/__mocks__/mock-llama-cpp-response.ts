import type {
  LlamaCPPResponse,
} from "contort";

export const makeLlamaCPPResponse = (response: Partial<LlamaCPPResponse> = {}): LlamaCPPResponse => ({
  content: 'foobar',
  id_slot: 42,
  model: "llama-1",
  prompt: "This is an example prompt.",
  stop: false,
  stopped_eos: false,
  stopped_limit: true,
  stopped_word: false,
  stopping_word: "stopword",
  tokens_cached: 100,
  tokens_evaluated: 400,
  tokens_predicted: 300,
  truncated: false,
  ...response,
  generation_settings: {
    dynatemp_exponent: 1.5,
    dynatemp_range: 10,
    frequency_penalty: 0.5,
    grammar: "default",
    ignore_eos: false,
    logit_bias: [0, 1, 2],
    min_keep: 5,
    min_p: 0.1,
    mirostat: 0.2,
    mirostat_eta: 0.01,
    mirostat_tau: 0.1,
    model: "llama-1",
    n_ctx: 1024,
    n_keep: 10,
    n_predict: 500,
    n_probs: 5,
    penalize_nl: true,
    penalty_prompt_tokens: ["example", "test"],
    presence_penalty: 0.4,
    repeat_last_n: 2,
    repeat_penalty: 1.2,
    samplers: ["top_k", "top_p"],
    seed: [123456, 789012],
    stop: ["\n"],
    stream: true,
    temperature: 0.7,
    tfs_z: 0.5,
    top_k: 50,
    top_p: 0.9,
    typical_p: 0.95,
    use_penalty_prompt_tokens: true,
    ...response.generation_settings,
  },
  timings: {
    predicted_ms: 1500,
    predicted_n: 300,
    predicted_per_second: 200,
    predicted_per_token_ms: 5,
    prompt_ms: 200,
    prompt_n: 50,
    prompt_per_second: 250,
    prompt_per_token_ms: 4,
    ...response.timings,
  },
});

